#!/usr/bin/env python3
"""AI Server ä¸»ç¨‹åºå…¥å£"""

import sys
import os
import glob
import traceback
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# å¯¼å…¥æ‰€æœ‰AIæ¨¡å—
from ai_core.tts.edge import EdgeTTS
from ai_core.asr.funasr_wrapper import FunASR
from ai_core.llm.chatglm import ChatGLM
from ai_core.audio.audio import DownlinkProcessor, UplinkProcessor

def get_performance_tips(inference_time, has_cuda):
    """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    tips = []
    
    # GPUç›¸å…³å»ºè®®
    if not has_cuda:
        tips.append("ï¿½ **GPUåŠ é€Ÿé€‰é¡¹**:")
        
        # Intel GPU ä¼˜åŒ–å»ºè®®
        tips.append("ğŸ”¹ **Intel Irisæ˜¾å¡ç”¨æˆ·**:")
        tips.append("   1. Intel XPUæ”¯æŒ (å®éªŒæ€§ï¼Œæå‡æœ‰é™ ~20-30%):")
        tips.append("      pip install intel-extension-for-pytorch")
        tips.append("      pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cpu")
        tips.append("   âš ï¸  æ³¨æ„: Intel GPUå¯¹AIæ¨¡å‹æ”¯æŒæœ‰é™ï¼Œæ•ˆæœä¸å¦‚NVIDIA GPU")
        
        tips.append("ğŸ”¹ **NVIDIA GPUç”¨æˆ· (æœ€ä½³é€‰æ‹©)**:")
        tips.append("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        tips.append("   ğŸš€ æ€§èƒ½æå‡: 5-10å€ (å¼ºçƒˆæ¨è)")
        
        tips.append("ğŸ”¹ **CPUä¼˜åŒ– (å½“å‰æ–¹æ¡ˆ)**:")
        tips.append("   - ç¡®ä¿ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ")
        tips.append("   - å…³é—­å…¶ä»–å ç”¨CPUçš„ç¨‹åº")
        tips.append("   - ä½¿ç”¨æ›´çŸ­çš„éŸ³é¢‘ç‰‡æ®µ")
    
    # é€Ÿåº¦åˆ†çº§å»ºè®®  
    if inference_time > 8.0:
        tips.append("ğŸŒ **æ¨ç†å¾ˆæ…¢** (>8ç§’):")
        tips.append("   - Intel Iris: é¢„è®¡æå‡20-30% â†’ çº¦6-7ç§’")
        tips.append("   - NVIDIA GPU: é¢„è®¡æå‡500-1000% â†’ çº¦1-2ç§’")
        tips.append("   - éŸ³é¢‘åˆ†å‰²: <30ç§’ç‰‡æ®µå¤„ç†")
    elif inference_time > 5.0:
        tips.append("ğŸŒ **æ¨ç†è¾ƒæ…¢** (5-8ç§’):")
        tips.append("   - Intel Iris: å¯å°è¯•ï¼Œä½†æå‡æœ‰é™")
        tips.append("   - å»ºè®®ä½¿ç”¨ç‹¬ç«‹NVIDIAæ˜¾å¡")
    elif inference_time > 2.0:
        tips.append("âš¡ **å¯ä¼˜åŒ–** (2-5ç§’):")
        tips.append("   - æ€§èƒ½å·²è¾ƒå¥½ï¼ŒGPUä¼˜åŒ–å¯é€‰")
    elif inference_time < 1.0:
        tips.append("ğŸ‰ **æ€§èƒ½ä¼˜ç§€** (<1ç§’)!")
        
    # Intel Iris ä¸“ç”¨å»ºè®®
    tips.append("ğŸ”· **Intel Irisæ˜¾å¡è¯´æ˜**:")
    tips.append("   - Intel GPUä¸»è¦ç”¨äºè§†é¢‘ç¼–è§£ç ï¼ŒAIæ¨ç†èƒ½åŠ›æœ‰é™")
    tips.append("   - FunASRç­‰è¯­éŸ³æ¨¡å‹å¯¹Intel GPUæ”¯æŒä¸å®Œå–„")
    tips.append("   - é¢„æœŸæ€§èƒ½æå‡: 20-30% (ç›¸æ¯”çº¯CPU)")
    tips.append("   - ä»å»ºè®®ä½¿ç”¨NVIDIA GPUè·å¾—æœ€ä½³æ€§èƒ½")
        
    # é€šç”¨ä¼˜åŒ–å»ºè®®
    tips.append("ğŸ“Š **é€šç”¨ä¼˜åŒ–**:")
    tips.append("   - æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶")
    tips.append("   - ä½¿ç”¨16kHzé‡‡æ ·ç‡éŸ³é¢‘")
    tips.append("   - ç›‘æ§CPU/GPUä½¿ç”¨ç‡")
    
    return tips

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸš€ AI Server")
    print("1. EdgeTTS æ¼”ç¤º")  
    print("2. FunASR æ¼”ç¤º")
    print("3. ChatGLM æ¼”ç¤º")
    print("4. Audio å¤„ç†æ¼”ç¤º")
    print("5. ç»¼åˆæ¼”ç¤º (Audio+ASR+LLM+TTS)")
    print("0. é€€å‡º")
    
    try:
        choice = input("\né€‰æ‹©åŠŸèƒ½: ").strip() or "1"
        
        if choice == "1":
            test_edge_tts()
        elif choice == "2": 
            test_funasr()
        elif choice == "3":
            test_chatglm()
        elif choice == "4":
            test_audio_processing()
        elif choice == "5":
            test_comprehensive_demo()
        elif choice == "0":
            print("é€€å‡º")
        else:
            print("æ— æ•ˆé€‰æ‹©")
            
    except Exception as e:
        print(f"è¿è¡Œå¤±è´¥: {e}")

def test_edge_tts():
    """EdgeTTSæµ‹è¯•"""
    try:
        tts = EdgeTTS.get_instance()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("outputs/tts", exist_ok=True)
        
        text = input("ğŸ¤ è¯·è¾“å…¥è¦åˆæˆçš„æ–‡å­— (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
        if not text:
            text = "ä½ å¥½ï¼Œè¿™æ˜¯EdgeTTSè¯­éŸ³åˆæˆæµ‹è¯•"
        
        result = tts.text_to_speech(text, "edge_test.mp3")
        
        if result:
            print(f"âœ… TTSæˆåŠŸ: {result}")
        else:
            print("âŒ TTSå¤±è´¥")
            
    except Exception as e:
        print(f"TTSå¤±è´¥: {e}")

def test_funasr():
    """FunASRæµ‹è¯•"""
    import time
    import torch
    
    try:
        print("ğŸ¤ FunASR è¯­éŸ³è¯†åˆ«æµ‹è¯•")
        print("=" * 40)
        
        # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
        print(f"ğŸ–¥ï¸  è®¾å¤‡ä¿¡æ¯:")
        print(f"   CPUæ ¸å¿ƒæ•°: {os.cpu_count()}")
        print(f"   CUDAå¯ç”¨: {'âœ…' if torch.cuda.is_available() else 'âŒ'}")
        if torch.cuda.is_available():
            print(f"   GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
            print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # åˆå§‹åŒ–ASRï¼ˆæµ‹é‡åˆå§‹åŒ–æ—¶é—´ï¼‰
        print(f"\nâ±ï¸  æ¨¡å‹åŠ è½½ä¸­...")
        start_time = time.time()
        asr = FunASR.get_instance()
        init_time = time.time() - start_time
        print(f"   åˆå§‹åŒ–è€—æ—¶: {init_time:.2f}ç§’")
        
        # æŸ¥æ‰¾æµ‹è¯•éŸ³é¢‘æ–‡ä»¶
        audio_files = glob.glob("outputs/tts/*.mp3")
        
        if not audio_files:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡ŒEdgeTTSæ¼”ç¤º")
            return
            
        audio_file = audio_files[0]
        # è·å–éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(audio_file) / 1024
        print(f"\nğŸ“ éŸ³é¢‘æ–‡ä»¶: {os.path.basename(audio_file)}")
        print(f"   æ–‡ä»¶å¤§å°: {file_size:.1f} KB")
        
        # æ‰§è¡ŒASRè¯†åˆ«ï¼ˆæµ‹é‡æ¨ç†æ—¶é—´ï¼‰
        print(f"\nğŸ”„ å¼€å§‹è¯†åˆ«...")
        start_time = time.time()
        result = asr.transcribe_file(audio_file)
        inference_time = time.time() - start_time
        
        print(f"âœ… è¯†åˆ«å®Œæˆ!")
        print(f"ğŸ¯ è¯†åˆ«ç»“æœ: {result}")
        print(f"â±ï¸  æ¨ç†è€—æ—¶: {inference_time:.2f}ç§’")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if result:
            chars_per_sec = len(result) / inference_time if inference_time > 0 else 0
            print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡: {chars_per_sec:.1f} å­—ç¬¦/ç§’")
        
        # è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œå»ºè®®
        print(f"\nï¿½ æ€§èƒ½åˆ†ææŠ¥å‘Š:")
        print(f"   ğŸ“ æ€»è€—æ—¶: {init_time + inference_time:.2f}ç§’ (åˆå§‹åŒ–: {init_time:.2f}s + æ¨ç†: {inference_time:.2f}s)")
        
        # è·å–æ€§èƒ½ç­‰çº§
        if inference_time < 1.0:
            level = "ğŸ‰ ä¼˜ç§€"
        elif inference_time < 2.0:
            level = "âœ… è‰¯å¥½"  
        elif inference_time < 5.0:
            level = "âš¡ ä¸€èˆ¬"
        else:
            level = "ğŸŒ è¾ƒæ…¢"
            
        print(f"   ğŸ† æ€§èƒ½ç­‰çº§: {level}")
        
        # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
        print(f"\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        tips = get_performance_tips(inference_time, torch.cuda.is_available())
        for tip in tips:
            print(f"   {tip}")
        
    except Exception as e:
        print(f"âŒ ASRå¤±è´¥: {e}")
        traceback.print_exc()

def test_chatglm():
    """ChatGLMæµ‹è¯•"""
    try:
        # æ£€æŸ¥APIå¯†é’¥
        api_key = os.getenv('ZHIPU_API_KEY')
        if not api_key:
            print("âŒ æœªé…ç½®ZHIPU_API_KEYç¯å¢ƒå˜é‡")
            print("ğŸ’¡ è¯·è®¾ç½®APIå¯†é’¥: export ZHIPU_API_KEY=your_api_key")
            return
        
        chatglm = ChatGLM.get_instance(api_key)
        
        # ç®€å•å¯¹è¯æµ‹è¯•
        user_input = input("ğŸ’¬ è¯·è¾“å…¥é—®é¢˜ (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
        if not user_input:
            user_input = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±"
        
        print("ğŸ¤– ChatGLMæ­£åœ¨æ€è€ƒ...")
        response = chatglm.generate_response(user_input)
        print(f"ğŸ¤– å›å¤: {response}")
        
    except Exception as e:
        print(f"ChatGLMå¤±è´¥: {e}")

def test_audio_processing():
    """Audioå¤„ç†æ¼”ç¤º"""
    try:
        
        print("ğŸµ Audio å¤„ç†æ¼”ç¤º")
        print("=" * 40)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("outputs/audio", exist_ok=True)
        
        # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
        audio_files = glob.glob("outputs/tts/*.mp3")
        if not audio_files:
            print("âŒ æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡ŒEdgeTTSæ¼”ç¤ºç”ŸæˆéŸ³é¢‘")
            return
        
        input_file = audio_files[0]
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
        
        # ä¸‹è¡Œå¤„ç†æ¼”ç¤º (TTS -> Opus)
        print("\nğŸ“¤ ä¸‹è¡Œå¤„ç† (TTSéŸ³é¢‘ -> Opusæ ¼å¼)")
        downlink = DownlinkProcessor(preset="balanced")
        opus_data = downlink.process_audio(input_file, output_format="bytes")
        print(f"   Opusæ•°æ®å¤§å°: {len(opus_data):,} bytes")
        
        # ä¸Šè¡Œå¤„ç†æ¼”ç¤º (Opus -> WAV)
        print("\nğŸ“¥ ä¸Šè¡Œå¤„ç† (Opusæ•°æ® -> WAVæ ¼å¼)")
        uplink = UplinkProcessor(preset="general")
        # ç¡®ä¿opus_dataæ˜¯bytesç±»å‹
        if isinstance(opus_data, bytes):
            output_file = uplink.decode_opus(opus_data, output_format="file")
        else:
            print("âŒ Opusæ•°æ®ç±»å‹é”™è¯¯")
            return
        print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°å¯¹æ¯”  
        original_size = os.path.getsize(input_file)
        opus_size = len(opus_data)
        # ç¡®ä¿output_fileæ˜¯å­—ç¬¦ä¸²è·¯å¾„
        if isinstance(output_file, str):
            decoded_size = os.path.getsize(output_file)
        else:
            decoded_size = 0
        
        print(f"\nğŸ“Š æ–‡ä»¶å¤§å°å¯¹æ¯”:")
        print(f"   åŸå§‹MP3: {original_size:,} bytes")
        print(f"   Opusç¼–ç : {opus_size:,} bytes (å‹ç¼© {(1-opus_size/original_size)*100:.1f}%)")
        print(f"   è§£ç WAV: {decoded_size:,} bytes")
        
    except Exception as e:
        print(f"Audioå¤„ç†å¤±è´¥: {e}")

def test_comprehensive_demo():
    """ç»¼åˆæ¼”ç¤º: Audio+ASR+LLM+TTS"""
    try:
        import os
        print("ğŸŒŸ AI Server ç»¼åˆæ¼”ç¤º")
        print("=" * 50)
        print("æµç¨‹: ç”¨æˆ·è¾“å…¥ -> TTS -> Audioå¤„ç† -> ASR -> LLM -> TTS")
        
        # æ£€æŸ¥APIå¯†é’¥
        api_key = os.getenv('ZHIPU_API_KEY')
        if not api_key:
            print("âŒ éœ€è¦ZHIPU_API_KEYç¯å¢ƒå˜é‡æ‰èƒ½è¿è¡Œç»¼åˆæ¼”ç¤º")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("outputs/comprehensive", exist_ok=True)
        
        # 1. ç”¨æˆ·è¾“å…¥
        user_text = input("ğŸ’¬ è¯·è¾“å…¥ä¸€ä¸ªé—®é¢˜ (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
        if not user_text:
            user_text = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½?"
        
        print(f"\nğŸ¯ ç”¨æˆ·é—®é¢˜: {user_text}")
        
        # 2. TTS - å°†ç”¨æˆ·é—®é¢˜è½¬ä¸ºè¯­éŸ³
        print("\nğŸ¤ æ­¥éª¤1: æ–‡å­—è½¬è¯­éŸ³ (TTS)")
        tts = EdgeTTS.get_instance()
        tts_result = tts.text_to_speech(user_text, "user_question.mp3")
        if not tts_result:
            print("âŒ TTSå¤±è´¥")
            return
        audio_path = tts_result
        print(f"   âœ… ç”Ÿæˆè¯­éŸ³: {audio_path}")
        
        # 3. Audioå¤„ç† - æ¨¡æ‹ŸIoTè®¾å¤‡ä¼ è¾“
        print("\nğŸ“¡ æ­¥éª¤2: éŸ³é¢‘ç¼–ç ä¼ è¾“ (Audio Processing)")
        
        # ä¸‹è¡Œ: ç¼–ç ä¸ºOpus
        downlink = DownlinkProcessor(preset="low_latency")
        opus_data = downlink.process_audio(audio_path, output_format="bytes")
        print(f"   ğŸ“¤ Opusç¼–ç : {len(opus_data):,} bytes")
        
        # ä¸Šè¡Œ: è§£ç ä¸ºWAVä¾›ASRä½¿ç”¨
        uplink = UplinkProcessor(preset="general")
        asr_audio_path = "outputs/comprehensive/for_asr.wav"
        # ç¡®ä¿opus_dataæ˜¯bytesç±»å‹
        if isinstance(opus_data, bytes):
            uplink.decode_to_file(opus_data, asr_audio_path)
        else:
            print("âŒ Opusæ•°æ®ç±»å‹é”™è¯¯")
            return
        print(f"   ğŸ“¥ è§£ç éŸ³é¢‘: {asr_audio_path}")
        
        # 4. ASR - è¯­éŸ³è¯†åˆ«
        print("\nğŸ¤ æ­¥éª¤3: è¯­éŸ³è¯†åˆ« (ASR)")
        import time
        asr = FunASR.get_instance()
        
        print("   â±ï¸ å¼€å§‹è¯†åˆ«...")
        start_time = time.time()
        recognized_text = asr.transcribe_file(asr_audio_path)
        asr_time = time.time() - start_time
        
        if not recognized_text:
            print("âŒ ASRå¤±è´¥")
            return
        print(f"   ğŸ¯ è¯†åˆ«ç»“æœ: {recognized_text}")
        print(f"   âš¡ ASRè€—æ—¶: {asr_time:.2f}ç§’")
        
        # 5. LLM - ç”Ÿæˆå›ç­”
        print("\nğŸ¤– æ­¥éª¤4: AIç”Ÿæˆå›ç­” (LLM)")
        chatglm = ChatGLM.get_instance(api_key)
        ai_response = chatglm.generate_response(recognized_text)
        print(f"   ğŸ’¡ AIå›ç­”: {ai_response}")
        
        # 6. TTS - å°†AIå›ç­”è½¬ä¸ºè¯­éŸ³
        print("\nğŸ”Š æ­¥éª¤5: å›ç­”è½¬è¯­éŸ³ (TTS)")
        tts_result = tts.text_to_speech(ai_response, "ai_response.mp3")
        if tts_result:
            print(f"   âœ… å›ç­”è¯­éŸ³: {tts_result}")
        
        print("\nğŸ‰ ç»¼åˆæ¼”ç¤ºå®Œæˆ!")
        print("ğŸ“‚ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ä¿å­˜åœ¨: outputs/comprehensive/")
        
    except Exception as e:
        print(f"ç»¼åˆæ¼”ç¤ºå¤±è´¥: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()